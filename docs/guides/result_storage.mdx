---
id: result-storage
title: Result storage
description: How to store and retrieve data with key-value store and dataset in Crawlee.
---

import ApiLink from '@site/src/components/ApiLink';

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

import DatasetBasicExample from '!!raw-loader!./code/result_storage_dataset_basic.py';
import KvStoreBasicExample from '!!raw-loader!./code/result_storage_kvs_basic.py';
import RsPurgeExplicitlyExample from '!!raw-loader!./code/result_storage_purge_explicitly.py';

This guide explains the available result storage options in Crawlee and how to use them to store and retrieve data.

Crawlee offers various types of result storage, each tailored for specific tasks. By default, data is saved to a local disk in the directory specified by the `CRAWLEE_STORAGE_DIR` environment variable. If this variable is not set, Crawlee will automatically use `./storage` within the current working directory.

The <ApiLink to="class/MemoryStorageClient">`MemoryStorageClient`</ApiLink> class manages Crawlee's storage. Throughout the crawler's execution, all data is held in memory, and it is simultaneously written to local files within their respective storage type folders.

## KeyValueStore

The <ApiLink to="class/KeyValueStore">`key-value store`</ApiLink> serves the purpose of saving and retrieving data records or files. Each data record is identified by a unique key and linked to a specific MIME content type. Key-value stores are particularly suited for storing web page screenshots, PDFs, or for maintaining the state of crawlers.

To open a key-value store, use the `open` class method with an ID, name, or optional configuration. If none are provided, the default store for the current crawler run is used. Attempting to open a non-existent store by ID will raise an error; however, if accessed by name, the store will be created if it doesnâ€™t exist. Data is typically in JSON format, though other formats are also supported.

The key-value store is represented by the KeyValueStore class. To facilitate easy access to the default key-value store, Crawlee provides the functions <ApiLink to="class/KeyValueStore#getValue">`KeyValueStore.get_value()`</ApiLink> and <ApiLink to="class/KeyValueStore#setValue">`KeyValueStore.set_value()`</ApiLink>.

By default, data is stored using the following path structure:
```text
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}
```
- `{CRAWLEE_STORAGE_DIR}`: The root directory for all storage data specified by the environment variable.
- `{STORE_ID}`: The identifier for the key-value store, either "default" or as specified by `CRAWLEE_DEFAULT_KEY_VALUE_STORE_ID`.
- `{KEY}`: The unique key for the record.
- `{EXT}`: The file extension corresponding to the MIME type of the content.

The following code demonstrates basic operations of key-value stores:

<CodeBlock className="language-python">
    {KvStoreBasicExample}
</CodeBlock>

To see a real-world example of how to get the input from the key-value store, see the [Screenshots](https://crawlee.dev/python/docs/examples/capture-screenshots-using-playwright) example.

## Dataset

<ApiLink to="class/Dataset">`Datasets`</ApiLink> are utilized for storing structured data, where each object contains the same attributes, like products from an online store or listings for real estate. You can visualize a dataset as a table, with each object represented as a row and its attributes as columns. Datasets function as append-only storage, allowing for the addition of new records but preventing modifications or deletions of existing ones.

Every Crawlee project run is linked to a default dataset, which is generally used to store the results specific to that crawler execution. Utilizing this dataset is optional.

In Crawlee, datasets are represented by the Dataset class. To facilitate writing to the default dataset, Crawlee provides the <ApiLink to="class/Dataset#pushData">`Datasets.push_data()`</ApiLink> function.

By default, data is stored using the following path structure:
```text
{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json
```
- `{CRAWLEE_STORAGE_DIR}`: The root directory for all storage data specified by the environment variable.
- `{DATASET_ID}`: Specifies the dataset, either "default" or a custom dataset ID.
- `{INDEX}`: Represents the zero-based index of the record within the dataset.

The following code demonstrates basic operations of the dataset:

<CodeBlock className="language-python">
    {DatasetBasicExample}
</CodeBlock>

## Cleaning up the storages

Default storages are purged before the crawler starts, unless explicitly configured otherwise. For that case, see <ApiLink to="class/Configuration#purge_on_start">`Configuration.purge_on_start`</ApiLink>. If you do not explicitly interact with storages in your code, the purging will occur automatically when the <ApiLink to="class/BasicCrawler#run">`BasicCrawler.run`</ApiLink> method is invoked.

If you need to purge storages earlier, you can call <ApiLink to="class/MemoryStorageClient#purge_on_start">`MemoryStorageClient.purge_on_start`</ApiLink> directly. This method triggers the purging process for the underlying storage implementation you are currently using.

<CodeBlock className="language-python">
    {RsPurgeExplicitlyExample}
</CodeBlock>